{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "autoencoders-VIDCHAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "225b83e8107f416f9ebb1e4ec951fac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2f71b6beb11c44c78775f6f4980f7d4d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b0bc545586774d229bd934abacbb3803",
              "IPY_MODEL_beee39e13a6944798d84c212b431a435"
            ]
          }
        },
        "2f71b6beb11c44c78775f6f4980f7d4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0bc545586774d229bd934abacbb3803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_63ca9e368db0483eaa82b606d198e963",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9887fa274a22468fa85eacaff0a4f5ab"
          }
        },
        "beee39e13a6944798d84c212b431a435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_151d8f1cbca64a0ca8c0cf8866219d13",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:06&lt;00:00,  1.52s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc1ed7cde5474475ab042bbc447cc4bf"
          }
        },
        "63ca9e368db0483eaa82b606d198e963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9887fa274a22468fa85eacaff0a4f5ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "151d8f1cbca64a0ca8c0cf8866219d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc1ed7cde5474475ab042bbc447cc4bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YwA2EdX0VdH"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as tfk\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfkl = tfk.layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28GQwMhxEsYw"
      },
      "source": [
        "### MNIST data\n",
        "Here is some code to load the MNIST digit recognition dataet and associated metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23CAr5Fl03MF",
        "outputId": "5bc9431a-ecdc-44f3-b0d6-57dbb0b03ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647,
          "referenced_widgets": [
            "225b83e8107f416f9ebb1e4ec951fac5",
            "2f71b6beb11c44c78775f6f4980f7d4d",
            "b0bc545586774d229bd934abacbb3803",
            "beee39e13a6944798d84c212b431a435",
            "63ca9e368db0483eaa82b606d198e963",
            "9887fa274a22468fa85eacaff0a4f5ab",
            "151d8f1cbca64a0ca8c0cf8866219d13",
            "cc1ed7cde5474475ab042bbc447cc4bf"
          ]
        }
      },
      "source": [
        "data, info = tfds.load('mnist', with_info=True)\n",
        "print(info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "225b83e8107f416f9ebb1e4ec951fac5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptioâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
            "tfds.core.DatasetInfo(\n",
            "    name='mnist',\n",
            "    version=3.0.1,\n",
            "    description='The MNIST database of handwritten digits.',\n",
            "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
            "    }),\n",
            "    total_num_examples=70000,\n",
            "    splits={\n",
            "        'test': 10000,\n",
            "        'train': 60000,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"@article{lecun2010mnist,\n",
            "      title={MNIST handwritten digit database},\n",
            "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
            "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
            "      volume={2},\n",
            "      year={2010}\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bevJSLhE_Lc"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's start by writting a preprocessing function.\n",
        "\n",
        "TensorFlow Datasets packages the MNIST data such that each element is a `dict` with two keys:\n",
        "- `image`: an array containing the image with shape (28, 28, 1), values of type `uint8`, and values between 0 and 255\n",
        "- `label`: An integer between 0 - 9 indicating the digit in the image\n",
        "\n",
        "Write a `preprocess` function that takes in one such element and prepares it for training an autoencoder. Things to make sure your function does:\n",
        "1. Cast the image to a `float32`\n",
        "2. Scale the values of the images so they are between 0 - 1\n",
        "3. Return a tuple with the transformed image and the label\n",
        "\n",
        "Then write another function called `ae_targets` that transforms this $(x, y)$ pair into the tuple that represents the input and target that are appropriate for an autoencoder.\n",
        "\n",
        "Finally, transform the originally dataset with the following steps:\n",
        "1. Grab the training dataset\n",
        "2. apply (i.e. \"map\") the preprocessing function\n",
        "3. apply the function to generate appropriate autoencoder targets\n",
        "3. cache the results (so you only to these tranformations the first time through the dataset)\n",
        "4. split the dataset into batches of size `32`\n",
        "5. set up the resulting dataset to repeat\n",
        "6. set up the resulting dataset to prefetch `5` elements\n",
        "\n",
        "(there is a chain of functions on the `Dataset` to do each of these!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aJJTNCR2VC1",
        "outputId": "23af52d8-cbab-40da-e17e-e22ed9041c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "# your code here\n",
        "preprocess = lamda d: (tf.cast(d['image']), tf.float32/255.,d['label'])\n",
        "ae_targets = lambda x,y: (x,x)\n",
        "\n",
        "ds_train =data['train'].map(preprocess).map(lambda x, y: (x,x)).cache().batch(32).repeat().prefetch(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-ab727220c925>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    preprocess = lamda d: (tf.cast(d['image']), tf.float32/255.,d['label'])\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpQgVaNCHzMO"
      },
      "source": [
        "### TensorFlow function API + a simple autoencoder\n",
        "\n",
        "We will want to use the encoder and decoder from our autoencoder separately down the road. Here is some code to build a simple autoencoder with a single hidden layer using the TensorFlow 'functional API' to see show we can combine two submodels (in this case the encoder and the decoder) into on larger model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoCZ3o72JYZY",
        "outputId": "022621c1-a79e-4758-c019-8361ff841c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "img_shape = info.features['image'].shape\n",
        "print(img_shape)\n",
        "print(np.prod(img_shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28, 1)\n",
            "784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSo1qWNXJTzH",
        "outputId": "3d088af5-c5cc-486b-e2e3-ef7f32a40002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "encoder = tfk.Sequential([\n",
        "  tfkl.InputLayer(img_shape),\n",
        "  tfkl.Flatten(),\n",
        "  tfkl.Dense(32, activation=tf.nn.relu)\n",
        "])\n",
        "encoder.summary()\n",
        "\n",
        "decoder = tfk.Sequential([\n",
        "  tfkl.InputLayer(32),\n",
        "  tfkl.Dense(np.prod(img_shape), activation=tf.nn.sigmoid),\n",
        "  tfkl.Reshape(img_shape)\n",
        "])\n",
        "decoder.summary()\n",
        "\n",
        "# the tf.keras function API starts with defining placeholder tensors that represent model inputs\n",
        "x = tfkl.Input(img_shape)\n",
        "\n",
        "# we can then pass this placeholder through our models to specify the computation to get a prediction\n",
        "h = encoder(x)\n",
        "xhat = decoder(h)\n",
        "\n",
        "# finally we use the tfk.Model class instantiate the model by specifying the inputs and outputs\n",
        "# (note: this can also be lists, which is how you make more complex models with multiple inputs and/or outputs)\n",
        "autoencoder = tfk.Model(inputs=x, outputs=xhat)\n",
        "autoencoder.summary()\n",
        "\n",
        "# training happens as usual\n",
        "autoencoder.compile(\n",
        "    optimizer=tfk.optimizers.Adam(),\n",
        "    loss=tfk.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "results = autoencoder.fit(ds_train, steps_per_epoch=400, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-414e879f911b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder = tfk.Sequential([\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtfkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtfkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtfkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_shape' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdpBTykAVbcL",
        "outputId": "5fc1f67a-4255-472d-d1c9-755a02434ae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "imgs = next(iter(ds_train))[0]\n",
        "for i in range(10):\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(imgs[i].numpy().squeeze(), cmap='bone')\n",
        "  plt.subplot(1, 2, 2, )\n",
        "  plt.imshow(autoencoder(imgs[i][tf.newaxis]).numpy().squeeze(), cmap='bone')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-72005ade7c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ds_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIGnE0CwVxXb"
      },
      "source": [
        "### Deep autoencoder and latent representations\n",
        "\n",
        "We want to reduce the dimensionality to the hidden layer so that we can plot the hidden representations to see how the autoencoder is organizing the data. If our hidden layers is going to be so much smaller, we will need to make up for it by making the network deeper.\n",
        "\n",
        "Modify the code above to make this work. Try a layers of size 156 -> 32 -> 2. After trining the model, use the encoder by itself and make a scatter plot of the some of the data, coloring each point by the digit identity. Which digits end up bunched together? Which are more well separated? Why might this be?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1t8p-Ak0VdN"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbG1-lOI7JzM"
      },
      "source": [
        "for i in range(10):\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(imgs[i].numpy().squeeze(), cmap='bone')\n",
        "  plt.subplot(1, 2, 2, )\n",
        "  plt.imshow(autoencoder(imgs[i][tf.newaxis]).numpy().squeeze(), cmap='bone')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7S9Aqn893jc"
      },
      "source": [
        "# get 1000 data points for the scatter plot\n",
        "x, y = next(iter(data['test'].map(preprocess).batch(1000)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TGMBWOW-0kG"
      },
      "source": [
        "# your code here -- run the data through the encoder and scatter plot the results"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}